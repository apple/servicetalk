include::ROOT:partial$component-attributes.adoc[]
= ServiceTalk Performance
This section will discuss the design, trade-offs, and performance tuning options and recommendations for your
ServiceTalk applications.

== Design goal
ServiceTalk grew out of the need for a Java networking library with the benefits of async-IOfootnote:[serving 10s to
100s of thousands of concurrent connections with a single application with small number of threads. This is impractical
to achieve with typical Java, Blocking-IO libraries that follow the 1 thread per connection networking model.] which can
hide the complexities of lower level networking libraries such as link:http://netty.io[Netty]. We also learned that
although asynchronous programming offers scaling benefits it comes with come complexity around control flow,
error propagation, debugging, and visibility. In practice there was a desire to mix synchronous
(for simplicity and developer velocity) with asynchronous (for improved utilization and vertical scalability) in the
same application.

ServiceTalk aims to hit a sweet spot by enabling applications to start simple with a traditional synchronous/blocking
programming model and evolve to async-IO as necessary. The value-proposition ServiceTalk offers, is an extensible
networking library with out of the box support for commonly used protocols (e.g. HTTP/1.x, HTTP/2.0, etc..) with APIs
tailored to each protocol layered in a way to promote evolving programming models (blocking/synchronous -> asynchronous)
without sacrificing on performance nor forcing you to rewrite your application when you reach the next level of scale.

ServiceTalk came from core Netty contributors footnote:[ServiceTalk is built on top of http://netty.io[Netty]]
recognizing the complexities of use (back-pressure, error propagation, RPC, EventLoop threading model, etc...) and
duplication (pooling, load balancing, resiliency, request/response correlation, etc...). We aim to provide a lightweight
networking library, with the best possible performance and minimal overhead in compute and GC pressure over Netty
(which is the link:https://netty.io/wiki/adopters.html[networking core for many large scale JVM deployments]). That
said, we may be making performance trade-offs in some areas to in favor of usability and/or safety. For example more
advanced concepts such as
link:https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html[Java Memory Model], reference counting, non-blocking
and asynchronous control flow.

== Trade-offs
As stated in the design goals, ServiceTalk aims to strike a balance between performance and usability, all while being
safe out of the box. This section clarifies some aspects, considerations and choices.

=== Blocking API on top of async
In order to build a library that supports both blocking and asynchronous APIs, we use an asynchronous core and build
blocking APIs on top. In case of our blocking APIs, it means there is some overhead involved to orchestrate the hand-off
(aka xref:servicetalk::blocking.adoc[offloading]) between EventLoop threads and application worker threads.

Network libraries that dedicate a single thread per connection to do IO and execute user code from that thread may have
better throughput and latency as long as the concurrent connection count is lowfootnote:[overhead of kernel/user-space
thread context switching will dominate CPU usage and there are limits in the 1000s of threads the OS wil allow an
application to start]. However there is an inflection point as concurrency increases to vertically scale which
may require rewriting the application to take advantage of end-to-end async-IO, or horizontally scale which requires
more capital investment. For this reason ServiceTalk may not be optimal in the low concurrency blocking user code
scenario, although our benchmarks have shown results within an acceptable threshold for most use cases despite this
architecture.

A strategy to avoid the thread hopping is to opt-in to executing user code on the EventLoop thread. However this will
have an adverse effect on latency and responsiveness if you execute xref:servicetalk::blocking.adoc[blocking] code. See
xref:servicetalk::evolve-to-async.adoc[Evolving to Asynchronous docs] for more details.

[#reference-counting]
=== Reference Counting
Java NIO and the Netty native API transports footnote:[https://netty.io/wiki/native-transports.html[epoll on Linux,
kqueue on Mac]] require direct (non-heap) memory
https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/nio/ByteBuffer.html[DirectByteBuffers] to read and
write bytes from the network driver. Allocating and collection of these buffers is very expensive, it may incur
application wide synchronization to allocate and puts additional pressure on the GC to clean up the
link:https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/ref/PhantomReference.html[PhantomReferences]
, which increases GC pauses.

image::ref-counting.png[GC overhead caused by DirectByteBuffers]

In order to overcome this, Netty provides
link:https://netty.io/4.1/api/index.html?io/netty/buffer/PooledByteBufAllocator.html[PooledByteBufAllocator] to amortize
the cost of the application's `DirectByteBuffer` overhead over the application's lifetime. In order for object pooling
to work the application needs to tell the pool when it's done with a resource, for this reason pooled Netty buffers
expose the concept of link:https://netty.io/wiki/reference-counted-objects.html[reference counting]. This reduces the
performance costs related to direct memory allocation/deallocation at the cost of complexity to the user who must manage
memory manually. Automatic memory management is a big selling point for the JVM and not something Java developers
have to think much about. From experience we've learned that manual memory management is challenging to get right. Even
if you are lucky enough to get it right it is a continuous cost you have to pay as the code base evolves and new members
of the team join. This can also lead to increased operational costs tracking down memory leaks or prematurely released
buffers. For this reason in ServiceTalk we've decided to not expose reference counted buffers to the user in APIs.
ServiceTalk will internalize the complexity of reference counting and minimize the costs of `DirectByteBuffer` costs
with various strategies (e.g. copying to heap buffers, using pooled allocators when buffers are not exposed to users,
etc...).

This trade-off of hiding complexity comes at the cost of performance. Exposing reference counting and leveraging buffer
pooling would provide more performance, but the performance trade-off was not large enough to justify the complexity for
most use cases. If your benchmarks indicate the lack of reference counting and buffer pooling is the culprit why you
cannot meet your performance SLA we suggest users to look into directly building on top of link:http://netty.io[Netty]
instead. At this level of scale you'll be most likely already using async non-blocking APIs, so the step up to Netty
will be small.

=== Back-pressure, Function Composition
In order to not overwhelm an async client or service, one needs to manage flow-control to give back-pressure.
Communicating flow control signals end to end footnote:[socket -- _read_ -> business logic -- _write_ -> socket] is
non-trivial in an async non-blocking application. ServiceTalk can enforce back-pressure at multiple layers
(e.g. number of connection, number of outstanding requests, per request, etc...) and utilizes
link:https://github.com/reactive-streams/reactive-streams-jvm/blob/v1.0.2/README.md#specification[Reactive Streams] APIs
to manage payload/data back-pressure throughout the core. `Reactive Streams` provides a means to handle streams of data
and allows the consumer of the data to specify how much data it is willing to consume in an asynchronous manner.
Much like the link:https://en.wikipedia.org/wiki/Futures_and_promises[`Future`/`Promise`] paradigm provides an means to
handle single-item (aka scalar) asynchronous operations, `Reactive Streams` provides a similar concept for multi-item
(aka streaming) asynchronous operations. Both concepts provide the ability to streamline asynchronous control flow and
error propagation via function composition instead of link:http://callbackhell.com[chaining callbacks], in
`Reactive Streams` this is commonly referred to as link:http://reactivex.io/documentation/operators.html[operators].

The streamlined control flow and end-to-end back-pressure comes with a trade-off which in practice typically involves
additional object allocation and as a side effect may lead to deeper stack traces. Another benefit of `Reactive Streams`
is a link:https://github.com/reactive-streams/reactive-streams-jvm#glossary[well defined] concurrency an interaction
pattern. For general operator implementations that represent asynchronous function composition these concurrency
constraints must be respected/enforced which may result in additional synchronization that may not be strictly required
if you take care of synchronization externally (e.g. operations are guaranteed to be on the same thread,
synchronization is enforced elsewhere in your control flow, etc...). In ServiceTalk we implement our own
operators (see xref:servicetalk-concurrent-api::index.adoc[Concurrent API]) in order to keep allocation and
synchronization low while strictly enforcing back-pressure. In practice the trade-off of streaming lined control flow
and automated back-pressure for performance was well worth it for most use cases. If your benchmarks indicate operator
implementations are the culprit why you cannot meet your SLA please
link:https://github.com/apple/servicetalk/issues/new[file an issue] describing your use case and we will work to improve
the operators in question.

[#safe-to-block]
=== Safe-to-block (aka Offloading)
Because ServiceTalk is asynchronous and non-blocking at the core it needs to defend against user or third party code
that potentially blocks IO EventLoop threads. Read the chapter on xref:blocking.adoc#safe-to-block[Blocking] for more
details.

`ServiceTalk` has xref:servicetalk-concurrent-api::blocking#executor-affinity[Executor Affinity] which ensures the same
`Executor` is used for each `Subscriber` chain on asynchronous operator boundaries. By default when using the streaming
APIs this requires wrapping on the asynchronous operator boundaries and may result in additional thread hops between
``Executor``s (and even if ``Executor`` happens to be the same). Depending upon the use case performance costs maybe
relatively high but we have the following compensatory strategies in place:

* The ServiceTalk team is investigating ways to reduce the cost of offloading for streaming APIs
* choosing the appropriate xref:servicetalk::performance.adoc#offloading-and-flushing[programming model] for your use
case allows us to be more optimal and reduce offloading
* you can opt-out of (some or all) offloading via
xref:servicetalk::performance.adoc#ExecutionStrategy[ExecutionStrategy]

== Tuning options and recommendations

Below sections will offer suggestions that may improve performance depending on your use-case.

[#offloading-and-flushing]
=== Programming model (offloading & flushing)
ServiceTalk offers APIs for various programming paradigms, to allow users to decide which programming model works best
for their use case. Different APIs also provide opportunities to optimize the amount of offloading and control when
flushing is required. Optimizations related to flushing and offloading can have a non-negligible impact on your
application's performance.

[#flushstrategy]
==== FlushStrategy
Flushing is the act of writing data that is queued in the Netty `Channel` pipeline to the network socket, typically
via link:http://pubs.opengroup.org/onlinepubs/9699919799/utilities/write.html[write] or
link:http://pubs.opengroup.org/onlinepubs/9699919799/functions/writev.html[writev]
link:https://en.wikipedia.org/wiki/System_call[syscall] on POSIX operating systems. Syscalls typically involve a user to
kernel-space context-switch which is relatively expensive. To compensate for this Netty introduces and intermediate
write queue to batch write operations triggered by flushing. This reduces the syscall frequency and if done effectively
should not have a negative impact on latency.

For example in benchmarks that involve a small number of writes (e.g. HTTP server responding to a `GET` request with
in memory content) reducing from 3 flushes to 1 flush almost tripled the throughput of the application. The general rule
of thumb is to try to batch writes as much as possible. You should consider your use case as if data is being generated
asynchronously (e.g. as a result to a call to another service) you may want to flush more frequently to avoid
introducing latency/responsiveness.

Exposing flushing on the public API is non-trivial when you have asynchronous control flow. The flush signals must be
ordered with respect to the data, and care must be taken not to drop these signals during data transformations.
ServiceTalk currently doesn't expose a way to control flush strategies in the public API but may be able to infer a more
optimal strategy if you select the appropriate programming paradigm for the
xref:servicetalk-http-api::index.adoc#client-programming-paradigms[client] and
xref:servicetalk-http-api::index.adoc#service-programming-paradigms[service]. If you are willing to use an
*_advanced, internal, experimental, subject to change at any time_* API there is also
link:{sourceroot}servicetalk-transport-netty-internal/src/main/java/io/servicetalk/transport/netty/internal/FlushStrategies.java[FlushStrategies]
that provides control over flushing. Here is a quick summary of this internal API.

[%header,cols="1,3,3"]
|===
|Strategy
|Description
|Use-case

|`flushOnEach()` *(default)*
|flushes after every item emitted on the write stream of a request/response (eg after the HTTP metadata, after every
payload chunk and after HTTP trailers)
|Typically what you want for a streaming application where every write needs to be delivered immediately.

|`flushOnEnd()`
|flushes only after the last item emitted on the write stream of a request/response (eg don't flush until the last HTTP
payload chunk or HTTP trailers)
|When your payload is aggregated, you most likely want to perform a single flush of the metadata + payload.

|`batchFlush(n, timeSource)`
|flush after `n` number of items are emitted or some time has elapsed (driven by the `timeSource`)
|This may be interesting if you have a high velocity streaming API, where you don't necessarily need to emit every item
individually and thus can batch a set of writes, with some control over the latency between flushes.

|===

`FlushStrategies` and related APIs are experimental and only exposed on the internal API by casting a
`ConnectionContext` to a `NettyConnectionContext` on a `Connection`. For example to update the strategy for an HTTP
client, for a single request one can do:

[source, java]
----
StreamingHttpClient client = HttpClients.forSingleAddress("localhost", 8080).buildStreaming();
StreamingHttpRequest request = client.get("/foo");

// Reserve a connection from the load-balancer to update its strategy prior to requesting
ReservedStreamingHttpConnection connection = client.reserveConnection(request)
        .toFuture().get(); // this blocks, for brevity in this example

// Update the strategy to "flush on each"
NettyConnectionContext nettyConnectionCtx = (NettyConnectionContext) connection.connectionContext();
nettyConnectionCtx.updateFlushStrategy((current, isOrig) -> FlushStrategies.flushOnEach());

connection.request(request);

// Release the connection back to the load-balancer (possibly restore the strategy before returning)
connection.releaseAsync().toFuture().get(); // this blocks, for brevity in this example
----
[CAUTION]
_`FlushStrategies` and related APIs are advanced, internal, and subject to change._

On the server side the strategy can be updated as part of the request/response, again by casting the context, or using a
`ConnectionAcceptorFilter` to set it once for all future requests on the same connection.

[source, java]
----
HttpServers.forPort(8080)
        .appendConnectionAcceptorFilter(delegate -> new ConnectionAcceptor() {
            @Override
            public Completable accept(final ConnectionContext ctx) {

                ((NettyConnectionContext)ctx).updateFlushStrategy((current, isOrig) -> FlushStrategies.flushOnEnd())

                return delegate.accept(ctx);
            }
        })
        .listenStreamingAndAwait((ctx, request, responseFactory) -> {

            ((NettyConnectionContext)ctx).updateFlushStrategy((current, isOrig) -> FlushStrategies.flushOnEnd())

            return Single.succeeded(responseFactory.ok()
                    .payloadBody(somePayload));
        });
----
[CAUTION]
_`FlushStrategies` and related APIs are advanced, internal, and subject to change._

[#ExecutionStrategy]
==== ExecutionStrategy (offloading)
link:{sourceroot}servicetalk-transport-api/src/main/java/io/servicetalk/transport/api/ExecutionStrategy.java[ExecutionStrategy]
is the core abstraction ServiceTalk uses to drive offloading delivering signals and data from the IO EventLoop threads.
For HTTP there is
link:{sourceroot}servicetalk-http-api/src/main/java/io/servicetalk/http/api/HttpExecutionStrategy.java[HttpExecutionStrategy]
which adds protocol specific offload points to be used by the clients and services. See
xref:servicetalk::performance.adoc#safe-to-block[Safe to Block] for more context into offloading and threading
models.

It is possible to override the `ExecutionStrategy`, but first make sure you are using the appropriate programming
paradigm for xref:servicetalk-http-api::index.adoc#client-programming-paradigms[client] and
xref:servicetalk-http-api::index.adoc#service-programming-paradigms[service]. Depending upon your protocol it is likely
there are higher level constructs such as xref:servicetalk-http-api::index.adoc#routers[routers] that provide a
per-route API customization (e.g.
xref:servicetalk-http-router-jersey::evolve-to-async.adoc[JAX-RS via Jersey] and
link:{sourceroot}servicetalk-http-router-predicate[Predicate Router]).

If you are using the appropriate programming model,
have reviewed xref:servicetalk-http-api::evolve-to-async.adoc[the docs on Evolving to Asynchronous], and are confident
you (or a library you use) will *not* execute blocking code in the path you are control flow being overridden, then
ServiceTalk allows you to override `ExecutionStrategy` at multiple levels:

1. Override per request (see xref:servicetalk-http-api::evolve-to-async.adoc#client[client] and
xref:servicetalk-http-api::evolve-to-async.adoc#server[server])
2. Override per client/server (see xref:servicetalk-http-api::evolve-to-async.adoc#client[client] and
xref:servicetalk-http-api::evolve-to-async.adoc#server[server])
3. Filters override
link:{sourceroot}servicetalk-http-api/src/main/java/io/servicetalk/http/api/HttpExecutionStrategyInfluencer.java[`HttpExecutionStrategyInfluencer`]
(or similar for your protocol) APIs

[IMPORTANT]
Disabling offloading entirely is an option that gives the best performance when you are 100% sure that none of your
code, library code or any ServiceTalk filters footnote:[Filters shipped with ServiceTalk, unless explicitly mentioned,
can be considered non-blocking] that are applied will block.

[#programming-models]
==== Choosing the optimal programming model
Selecting the appropriate programming paradigm can help simplify your application logic (see
xref:servicetalk-http-api::index.adoc#client-programming-paradigms[client programming paradigms] and
xref:servicetalk-http-api::index.adoc#service-programming-paradigms[service programming paradigms]) and also
enables ServiceTalk to apply optimizations behind the scenes (e.g.
xref:servicetalk::performance.adoc#flushstrategy[flushing] and
xref:servicetalk::performance.adoc#ExecutionStrategy[offloading]). The paradigm is chosen when constructing the client
or server, transforming a client on demand on a per-request basis (e.g.
link:{sourceroot}servicetalk-http-api/src/main/java/io/servicetalk/http/api/HttpClient.java#L77-L79[HttpClient#asBlockingClient()]
), or levering a service xref:servicetalk-http-api::index.adoc#routers[router's] per-route ability to support the
different paradigms. The following table is a summary of how the programming paradigm affects flushing and offloading.
Please consider reading the detailed documentation on
xref:servicetalk-http-api::blocking.adoc#programming-models[HTTP Programming models].

[%header,cols="1,3,3,3,3"]
|===

|Model
|Flush
|Offload Server
|Offload Client
|Use-case

|*Async Aggregated*

`cb.build()`

`sb.listen()`
|Single Flush
|Offload handling the request (Meta + payload combined)

Offload the response control signals
|Offload handling the response (Meta + payload combined)


|you have aggregated data and your code can deal with `Single<T>` or `Future<T>`

|*Async Streaming*

`cb.buildStreaming()`

`sb.listenStreaming()`
|Flush Meta +
Flush Each Item
2+|Offloads receipt of Meta, every payload item and all control signals

|you have streaming data and your code can deal with `Single<T>` or `Future<T>` and `Publisher<T>`

|*Blocking Aggregated*

`cb.buildBlocking()`

`sb.listenBlocking()`
|Single Flush
|Offload handling the request (Meta + payload combined)
|None
|you have aggregated data and blocking code

|*Blocking Streaming*

`cb.buildBlockingStreaming()`

`sb.listenBlockingStreaming()`
|Flush Meta +
Flush Each Item
|Offload receipt of Meta
|Offload control signals
|you have streaming data and blocking code

|===

This table clarifies how merely choosing the _programming model_ depending on your use-case can improve efficiency. If
you can in addition completely opt-out of the offloading (consult with
xref:servicetalk::performance.adoc#ExecutionStrategy[offloading]), you will get the best possible performance.

[#jersey-programming-models]
==== JAX-RS Jersey Router Programming Model
Choosing the right programming model can have significant performance benefits when deploying Jersey routes as well. All
Jersey APIs are supported under all models, however there may be some unexpected side-effects, for example when choosing
an _Aggregated_ router implementation. You would still be able to use streaming data types footnote:[`Reactive Streams`
or `Input|OutputStream`] as input and output for JAX-RS endpoints, but need to realize that there will be buffering
behind the scenes to aggregate and deliver the data in a single payload when the stream completes.

[NOTE]
There is no API-wise need for the Jersey router to be implemented in the 4 different programming models, however it
currently offers the most effective way to benefit from these performance optimizations and may improve this in the
future.

[%header,cols="1,4"]
|===

|Model
|Optimal use-case

|Async Aggregated
|`Single<T>`, `Publisher<T>`, `Completable` data types in endpoints with aggregated data.

best performance with offloading disabled for aggregated use-cases, optionally using ServiceTalk
serializers

|Async Streaming
|`Publisher<T>` data type in endpoints with streaming data

best performance with offloading disabled for streaming use-cases, optionally using ServiceTalk serializers.

|Blocking Aggregated
|typical primitive and aggregated JAX-RS data types, `Buffer`, `byte[]` or POJOs with serialization

best performance in general when endpoints have aggregated data

|Blocking Streaming
|best performance when endpoint depend on `InputStream` and `OutputStream`

|===

[TIP]
When in doubt using _Blocking Aggregated_ or _Blocking Streaming_ is a safe bet to get good performance, especially if
you are converting an existing vanilla JAX-RS application.

If you need to mix `Reactive Streams` routes with typical JAX-RS _Blocking Aggregated_ routes, you have 2 options.
Either you'll fall back to the _Async Streaming_ model to avoid aggregating your streams and lose some optimizations for
your Blocking Aggregated routes. Or if your paths allow it, you can front-load your Jersey Router with the ServiceTalk
Predicate Router and compose 2 Jersey routes behind the Predicate Router, each in their respective optimal programming
model.


=== IO Thread pool sizing
By default ServiceTalk size the IO Thread-pool as follows:

[source, java]
----
2 * Runtime.getRuntime().availableProcessors()
----
[NOTE]
====
Available processors: CPU cores (logical
https://en.wikipedia.org/wiki/Hyper-threading[Simultaneous Multithreading (SMT)] cores if available) or container compute
units as defined by https://en.wikipedia.org/wiki/Cgroups[Linux cgroups]
====
The number of IO threads generally correlates to the number of available processors because this is how much logical
concurrency available to your application. The IO threads are shared across connections and even requests the number of
IO threads is not directly related to the number of requests. If you read and understand the consequences of disabling
xref:servicetalk::performance.adoc#ExecutionStrategy[offloading] then your business logic will execution directly on an
IO thread. As your business logic consumes more processing time (e.g. CPU cycles, blocking calls, etc...) it may be
beneficial to have more than just `Runtime.getRuntime().availableProcessors()` threads. However the more processing time
you take for a single request/connection, the more latency is incurred by other connections which share the same IO
thread. You should also consider that more threads generally means more context switches. Like anything performance
related your mileage may vary and you should benchmark your specific use case.

In benchmarks which deal in memory data and consume minimal processing time (e.g. HTTP/1.x server responding to `GET`
request with in memory payload, no compression, encryption, etc...) setting the number of IO threads the equal to number
of logical SMT cores gave the best performance and was ~10% better than `2 * Runtime.getRuntime().availableProcessors()`

For example, to override the IO Thread pool on an HTTP client builder (equivalent on the server builder):

[source, java]
----
IoExecutor ioExecutor = NettyIoExecutors.createIoExecutor(
                Runtime.getRuntime().availableProcessors(),
                new IoThreadFactory("io-pool"));

HttpClients.forSingleAddress("localhost", 8080)
                .ioExecutor(ioExecutor)
                .buildStreaming();
----

=== Socket and Transport Options
ServiceTalk exposes configuration knobs at various layers of the stack. At the lowest layer there are the TCP
`SocketOptions` and ServiceTalk options, both exposed on the client builder.

[source, java]
----
BlockingHttpClient client = HttpClients.forSingleAddress("localhost", 8080)
        .socketOption(StandardSocketOptions.SO_RCVBUF, 1234567)
        .socketOption(StandardSocketOptions.SO_SNDBUF, 1234567)
        .socketOption(ServiceTalkSocketOptions.CONNECT_TIMEOUT, 12345)
        .socketOption(ServiceTalkSocketOptions.IDLE_TIMEOUT, 12345L)
        .socketOption(ServiceTalkSocketOptions.WRITE_BUFFER_THRESHOLD, 12345)
        .buildBlocking();
HttpResponse resp = client.request(client.get("/"));

----

=== HTTP Service auto payload-draining
If a user forgets to consume the request payload (e.g. returns an `HTTP 4xx` status code and doesn't care about the
request payload) this may have negative impacts on subsequent requests on the same connection:

* HTTP/1.x connections may have multiple serial requests and we cannot read the next request until the current request
is consumed.
* HTTP/2.0 connections have flow control on each stream, and we want to consume the payload to return the bytes to flow
control

To avoid these issues, ServiceTalk HTTP servers will automatically drain the request payload content after the response
is sent. However this adds some additional complexity to the HTTP service control flow in ServiceTalk and adds some
overhead. If you know for sure that the payload is always consumed footnote:[typically compose the response with
`request.payloadBody().ignoreElements()`], or you are not using the streaming APIs, this mechanism can be disabled to
save some CPU and memory as follows:

[source, java]
----
HttpServers.forPort(8080)
                .disableDrainingRequestPayloadBody()
                .listenStreamingAndAwait((ctx, request, responseFactory) -> ..);
----

=== HTTP Header validation
ServiceTalk aims to be safe by default, hence it validates HTTP headers (including cookies) in accordance to the
link:https://tools.ietf.org/html/rfc7230[HTTP RFCs]. However validation is not for free and comes with some overhead. If
you know that your headers will always be valid or are willing to forgo validation, then you can disable header
validation as follows:

[source, java]
----
DefaultHttpHeadersFactory headersFactory = new DefaultHttpHeadersFactory(false /* names */,
                                                                         false /* cookies */);

HttpClients.forSingleAddress("localhost", 8080)
                .headersFactory(headersFactory)
                .buildBlocking();
----

=== AsyncContext
In traditional sequential programming where each request gets its own dedicated thread Java users may rely upon
`ThreadLocal` to implicitly pass state across API boundaries. This is a convenient feature to take care of cross
cutting concerns that do not have corresponding provisions throughout all layers of APIs (e.g.
link:https://www.slf4j.org/manual.html#mdc[MDC], auth, etc...). However when moving to an asynchronous execution model
you are no longer guaranteed to be the sole occupant of a thread over the lifetime of your request/response processing,
and therefore `ThreadLocal` is not directly usable in the same way. For this reason ServiceTalk offers
xref:servicetalk-concurrent-api::async-context.adoc[`AsyncContext`] which provides a static API similar to what
`ThreadLocal` provides in one-request-per-thread execution model.

This convenience and safety comes at a performance cost. Intercepting all the code paths in the asynchronous control
flow (e.g. async operators) requires wrapping to _save_ and _restore_ the current context before/after the
asynchronous control flow boundary. In order to provide a static API `ThreadLocal` is also required, although an
optimization (e.g.
link:{sourceroot}servicetalk-concurrent-api/src/main/java/io/servicetalk/concurrent/api/AsyncContextMapHolder.java[AsyncContextMapHolder]
) is used to minimize this cost. This `ThreadLocal` optimization is enabled by default and can be enabled by using our
link:{sourceroute}servicetalk-concurrent-api/src/main/java/io/servicetalk/concurrent/api/DefaultThreadFactory.java[DefaultThreadFactory]
if you use a custom `Executor`.

In benchmarks with high throughput and asynchronous operators you will likely see a drop in throughput.
Like most common features in ServiceTalk this is enabled by default and can be opted-out of as follows:

[CAUTION]
Some ServiceTalk features such as `OpenTracing` may depend on `AsyncContext`.

[source, java]
----
static {
    // place this at the entrypoitn of your application
    AsyncContext.disable();
}
----

=== Netty PooledByteBufAllocator
ServiceTalk leverages Netty's
link:https://netty.io/4.1/api/index.html?io/netty/buffer/PooledByteBufAllocator.html[PooledByteBufAllocator] internally
in cases where we have scope on the xref:servicetalk::performance.adoc#reference-counting[reference counted] objects
and can ensure they won't leak into user code. The `PooledByteBufAllocator` itself has some configuration options
that we currently don't expose. There are some internal system properties exposed by Netty which can be used to tweak
the default configuration. Note these are not a public API from ServiceTalk's perspective and are subject to change
at any time. For more info checkout the
link:https://www.facebook.com/notes/facebook-engineering/scalable-memory-allocation-using-jemalloc/480222803919[jemalloc inspired buffer pool]
and the
link:https://netty.io/4.1/api/index.html?io/netty/buffer/PooledByteBufAllocator.html[PooledByteBufAllocator source].
Here are a few options for motivation:

[%header,cols="1,3a"]
|===
|Option
|Description


|`io.netty.allocator.numHeapArenas`
|Number or arenas for HEAP buffers, this impacts how much system memory is reserved for buffer pooling.
[TIP]
Unused by `ServiceTalk`, set this to `0`, unless it can't use Direct Buffers and needs to fall back to HEAP

|`io.netty.allocator.numDirectArenas`
|Number or arenas for Direct buffers, this impacts how much system memory is reserved for buffer pooling

|===

=== netty-tcnative OpenSSL engine
SSL encryption can cause significant compute overhead over non-encrypted traffic. The
link:https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/SSLEngine.html[SSLEngine] for commonly used JDK8
distributions are not known for having the best performance characteristics relative to alternative SSL implementations
available in other languages (e.g. OpenSSL). The
link:https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/SSLEngine.html[SSLEngine] OpenJDK performance has improved
in JDK11 but still may not give comparable performance relative to alternative SSL implementations (e.g. OpenSSL). For
this reason the Netty team created
link:https://netty.io/wiki/forked-tomcat-native.html[netty-tcnative]
based upon OpenSSL footnote:[BoringSSL is a drop-in replacement for OpenSSL maintained by Google] which is a production
ready `SSLEngine` implementation. Using `netty-tcnative` with ServiceTalk is as easy as dropping in the
link:https://netty.io/wiki/forked-tomcat-native.html#artifacts[JAR] of the SSL implementation on your classpath.

You should also investigate the configuration of SSL which may impact performance. For example selecting the cipher
suite and encryption/link:https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake[handshake]/
link:https://en.wikipedia.org/wiki/Message_authentication_code[MAC] algorithms may have an impact on performance if you are able to
link:https://en.wikipedia.org/wiki/AES_instruction_set[hardware acceleration]. Performance shouldn't be the only
consideration here as you must consider the security characteristics and what protocols your peers are likely to support
(if they are out of your control). It is recommended to consult reputable resources (such as
link:https://wiki.mozilla.org/Security/Server_Side_TLS[Mozilla Server Side TLS]) to learn more.

[source, java]
----

// add the netty dependency to your build, eg: "io.netty:netty-tcnative-boringssl-static:2.0.25.Final"

BlockingHttpClient client = HttpClients.forSingleAddress("netty.io", 443)
        .sslConfig(
                SslConfigBuilder.forClient("netty.io", 443)
                        .provider(SslConfig.SslProvider.OPENSSL) // SslProvider.JDK | SslProvider.AUTO
                        .build())
        .buildBlocking();
HttpResponse resp = client.request(client.get("/"));
----

=== Netty LEAK-detection
ServiceTalk is built on top of Netty. Netty supports
xref:servicetalk::performance.adoc#reference-counting[reference counting] of `ByteBuf` objects (reference counting
is xref:servicetalk::performance.adoc#reference-counting[not exposed] by ServiceTalk). To help debug reference counting
related bugs Netty provides a
link:https://netty.io/wiki/reference-counted-objects.html#leak-detection-levels[leak detector for `DirectByteBuffers`].
The default `SIMPLE` detector has a relatively small overhead (intended to be used in production) by sampling a small
subset of buffer allocations and add additional tracking information. This overhead can be avoided at the risk of
less visibility into reference counting bugs as follows:

[CAUTION]
This reduces visibility on reference counting bugs in ServiceTalk and Netty. This is not a public API exposed by
ServiceTalk and is subject to change at any time.

[source]
----
-Dio.netty.leakDetection.level=DISABLED
----

== Benchmarks
While we are careful in not adding unnecessary performance overhead during the development of ServiceTalk, we can't
claim we deliver on this goal unless we measure. Therefore we've developed a performance test bench that we run
periodically. This section will describe our test environment and some of the scenarios that we put to the test.
Currently the performance test-suite is not publicly available, but we intend to incrementally add tooling and
benchmarks to the code-base. Every environment and use-case is different and may perform differently and requires
different tuning strategies, so we want to enable our users to easily test and confirm the performance of ServiceTalk
meets their needs.

=== Environment
[cols="1,3a"]
|===
| CPU
| Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz (3.40GHz with Turbo)

| NUMA Nodes
| 2 processors

| CPU Cores
| 16 (2x 8 physical CPU cores, 32 logical CPU cores with HyperThreading)

| CPU Cache
| 20MB

| Memory
| 250GB

| OS
| CentOS 6.9 -- Linux 4.15

| Network
| N/A -- all tests performed using `localhost` networking

| Virtualization
| N/A -- bare metal

|===

=== Reducing the noise
In order to minimize noise in the benchmarks we mitigate the problem from different angles

* the system has 2 link:https://en.wikipedia.org/wiki/Non-uniform_memory_access[NUMA nodes] which allows us to isolate
the load-generator from the system under test
* background and operational processes typically run on the first few logical CPUs (first NUMA node), these don't
consume much resources but we don't want the system under test to compete for resources and be negatively affected by
cache invalidation by context switching
* using `localhost` networking avoids bottlenecks and issues on the physical network, potential network virtualization
overhead, including misconfiguration and noisy neighbors. This way we ensure that the only latency observed is the one
introduced by ServiceTalk.
* for server benchmarks the client load generator requires less compute cycles, so we co-locate it on the 1st NUMA-node,
shared with the background processes but confined to a set of logical CPUs that are rarely used by the background
processes. The server itself will be dedicated to all the cores of the 2nd NUMA-node.
* for client benchmarks the background server taking request is a minimal Netty-based application. We use Netty and
not ServiceTalk because we don't want to worry about the SerivceTalk service to be the bottleneck. The Netty server is
co-located with the background processes on the first NUMA-node, but again isolated from the cores typically receiving
most background tasks. The client under test will be dedicated to all the cores of the 2nd NUMA-node.
* every scenario test is performed from a clean start of either the client and server and repeated 3 times, for a
duration of 3 minutes. This gives us sufficiently consistent numbers and guarantees the bench goes through a phase of
JIT warm-up, some GC while working and a steady state. The throughput numbers are averaged over the runs.

=== Test scenarios
We obviously can't test all scenarios, but our aim is to continuously monitor performance of a set of use cases that
are representative of real world use cases while also isolating ServiceTalk as much as possible (e.g. not too much
business logic). In addition we'll also compare how well other libraries and frameworks in the Java ecosystem perform,
for example it's interesting for us to compare against Netty, as it shows us exactly how much overhead we are adding on
top.

==== Clients and Server types
* HTTP Clients and Servers in all programming models (see
xref:servicetalk::performance.adoc#programming-models[programming models] for performance implications)
    ** Async Aggregated
    ** Async Streaming
    ** Blocking Aggregated
    ** Blocking Streaming

* JAX-RS Jersey router performance
    ** all xref:servicetalk::performance.adoc#jersey-programming-models[Jersey Router programming models]
    ** common JAX-RS data types (`String`, `byte[]`, `InputStream`, `OutputStream`)
    ** Reactive Streams types (`Single<T>`, `Publisher<T>`, `Publisher<Buffer>`)
    ** JSON with Jersey Jackson module & `ServiceTalk` Jersey Jackson module

==== Features and dimensions
* PLAIN vs SSL
* offloading (default) vs not-offloading
* HTTP Methods
    ** GET
    ** POST
* Payload sizes
    ** 0
    ** 256
    ** 16KB
    ** 256KB
* AsyncContext enable/disable
* Header validation enable/disable
* IO Thread count
* Connection count

=== Load-generators
In case of testing the HTTP server, we use either the native HTTP load-generator link:https://github.com/wg/wrk[wrk]
written in C with async IO, or a purpose built ServiceTalk load-generator using the ServiceTalk asynchronous HTTP
client. Our custom load-generator reports within 1-3% of the same numbers as `wrk`. We use our load-generator to test
scenarios that `wrk` doesn't support such as streaming data, SSL, and possibly non-HTTP protocols in the future. By
baselining our load-gen against `wrk` increases confidence in the results and may help identify regressions or
improvements in ServiceTalk skewing the numbers. Similarly for testing client performance we use a Netty-based server
implementation for reference as it avoids any overhead ServiceTalk may add and can be used to baseline in the same
fashion to the client load-gen, in case we need a backend server with features only available in ServiceTalk.

==== wrk -- native HTTP client -- target 1000 connections
Wrk opens 1000 connections to the target, then using an thread-pool of 8 workers (it has 8 dedicated CPU cores) it fires
requests at the target server, each time a response completes on a connection, it fires the next request, and so on
until the test completes, typically 3 minutes.

==== ServiceTalk Async HTTP Client bench -- target 1000 connections
In a loop 1000 requests are fired using the Async HTTP Client, which opens roughly 1000 connections, then from its IO
thread-pool of 16 workers (it has 16 dedicated CPU cores) it fires new requests each time a response completes on a
connection, and so on until the test completes, typically 3 minutes.

==== ServiceTalk Blocking HTTP Client bench -- target 1000 connections
Starts a work-pool of 1000 threads (it has 16 dedicated CPU cores) then from every worker thread it uses a Blocking HTTP
Client to send a request and blocks on the response. When the response completes, it fires the next request and blocks
again, and so on until the test completes, typically 3 minutes.

=== Latency reporting
So far we've mostly talked about throughput, but it's often interesting to report on latency as well, both `wrk` as well
as our custom load-generator support latency reporting -- we leverage the awesome
link:https://github.com/HdrHistogram/HdrHistogram[HdrHistogram] to make sure we're striking a good balance between
throughput and latency.

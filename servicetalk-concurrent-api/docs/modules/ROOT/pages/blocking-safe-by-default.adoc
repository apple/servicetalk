// Configure {source-root} values based on how this document is rendered: on GitHub or not
ifdef::env-github[]
:source-root:
endif::[]
ifndef::env-github[]
ifndef::source-root[:source-root: https://github.com/apple/servicetalk/blob/{page-origin-refname}]
endif::[]

= Blocking safe by default

This document elaborates how asynchronous sources provide ways to offload. General philosophy of writing blocking code
while using ServiceTalk is explained xref:{page-version}@servicetalk::blocking-safe-by-default.adoc[here] and is a recommended read.


== Execution chain

Any ServiceTalk asynchronous source (`Publisher`, `Single` and `Completable`) offers multiple operators that can be
applied to the source. Any instance of such a source and all the operators applied to it before it is subscribed, is
called an execution chain.

=== Data and control flow in an execution chain

The image below details the flow of data (`Subscriber` methods) and control (`Publisher` and `Subscription` methods)
messages. Invocations in either direction may be executed on an event loop thread and hence needs to be protected. In
case new asynchronous sources are generated/received inside operators, they follow the same model and hence are removed
for brevity.

image::blocking-scenarios.svg[Data and control flow in an execution chain]

As shown in the above picture, there are inherently two directions (data and control) of information flow for an
execution chain and these
link:https://github.com/reactive-streams/reactive-streams-jvm/blob/v1.0.3/README.md#glossary[signals] can be triggered
in parallel.

**By default, in ServiceTalk, signals are not executed on an event loop thread, but instead executed using an
link:{source-root}/servicetalk-concurrent-api/src/main/java/io/servicetalk/concurrent/api/Executor.java[Executor]
provided by the application in the order they are received.**

==== Implications

The implication of the above approach is the following:

**Users can execute blocking code inside an execution chain, provided they are not waiting for another data or control
message in the same execution chain.**

== Task Offloading
ServiceTalk uses Netty for network I/O and the Netty implementation uses a fixed number of I/O threads for executing
network operations. The number of EventLoop threads correlates to the number of CPU cores and *not* to the number of
requests. This is because in many cases threads typically sit idle while waiting for I/O to complete. Sharing threads
helps minimize resource consumption and improves scalability. However, sharing threads for different requests means all
control flow for a single request will impact all other requests that share the same thread. If the control flow blocks
the current thread for a longer time period (aka "blocks the thread") (e.g. external I/O) this may negatively impact
latency and throughput. One approach to ensure I/O thread availability is to carefully limit the scope of work done by
I/O threads and, whenever practical, delegate all other necessary tasks that are not related to I/O to some other
thread. Moving tasks from I/O threads to other threads is called “offloading” and is a core technique used by
ServiceTalk.

Offloading is used for two purposes within ServiceTalk; firstly for the
execution needed for the handling of asynchronous events, aka
link:https://github.com/reactive-streams/reactive-streams-jvm/blob/v1.0.3/README.md#glossary[Signals],
and secondly when handling asynchronous events, protecting scarce resources from being monopolized by blocking,
untrusted or expensive application code. Asynchronous execution requires offloading – the initiating application thread
is not available. The use of protective offloading, where offloading is used for avoid resource monopolization, is a
practical consideration, but just as necessary for reliable and predictable operation.

ServiceTalk will, by default, execute most application code on threads other than the Netty I/O threads.
For most invocations of application code, if the application developer knows that their code cannot block and always
executes quickly in near constant time they can request that ServiceTalk not offload their code. This will improve
application performance by reducing latency and overhead. Requests to not offload will be honored by ServiceTalk if all
the other components in the same execution path have also opted out of offloading. See xref:{page-version}@servicetalk-concurrent-api::blocking-safe-by-default.adoc#execution-strategy[Execution Strategy]
for more information on how components may specify their offloading requirements. As a last resort, tasks may also be
queued to be performed as threads are available.

ServiceTalk is designed to be fully asynchronous except where the API provides explicit blocking behavior xref:{page-version}@servicetalk::programming-paradigms.adoc[as a convenience].

ServiceTalk uses a task based approach for offloading, using `Executor` in the standard "fire-and-forget" way to run the
offloaded tasks. Often the `Executor` has a pool of threads, possibly unbounded, and tasks are run using whatever thread
is available. In particular, different threads may be used for each task executed and code running in tasks cannot
depend upon a consistent thread being used for invoking program logic. This approach is generally the most scalable
because it makes the best utilization of threads. If it is necessary to share state between tasks then
`link:{source-root}/servicetalk-concurrent-api/src/main/java/io/servicetalk/concurrent/api/ContextMap.java[ContextMap]`s
can be used.

== Implementation

In order to use ServiceTalk's blocking support feature, one does not need to know about implementation details and the
above information is sufficient. However, if you are developing some operators in ServiceTalk or are just curious,
xref:blocking-implementation.adoc[blocking-implementation.adoc] describes the design.
